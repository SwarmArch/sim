Simulator User Guide
====================

Here we discuss how to write programs for Swarm, how to invoke the simulator,
as well as how to access and interpret the simulator's output.

Invoking the simulator
----------------------

The way to run the simulator is to run a command like this:
```
$ build/opt/sim/sim -config path/to/configuration/file.cfg -- [insert command to run the application here]
```
where the configuration (.cfg) file specifies the hardware configuration
(i.e., number of cores, number and size of caches, and lots of other parameters).
We have provided a [sample configuration file](../sample.cfg) with simple defaults
appropriate for quickly testing the basic functionality of the simulator.
Files reflecting the configuration of the system used in actual experiments,
such as those described in published papers, can be found in other repositories.

It is possible to run most sorts of applications on the simulator,
as long as they consist of a single process and can be invoked
as a single executable x86/64 binary, with any number of command line arguments.
These programs must usually be instrumented so the simulator knows what
period of execution is to be simulated.
(The simulator fast-forwards until the start of the period of interest,
then simulates the period of interest to produce simulation results,
and can return to fast-forwarding at the end of the period of interest.)

After running the simulator, the next section of this document
provides information on using simulation output.

Interpreting simulation results
-------------------------------

When you run the simulator, any lines printed by the simulator are prefixed with `[sim] `.
(Other lines you see are printed by the application program.)
The lines printed to the terminal by the simulator include a brief summary of the results.

The top-line metric you should care about is the running time of the Swarm tasks,
which is given by the line "Done at cycle ..." in the standard output stream from the simulator,
and by the line in sim.out that looks like:
```
 cycles: XXXXX # Simulated cycles
```

The output printed to the simulator further breaks down some important statistics
according to the different task functions in the program,
and tells you how many times a task started execution (was dispatched from a task queue).
Each such execution of a task is sometimes also confusingly called a "task", especially in the statistics.
So, if a task starts running once, and is aborted, and later re-executes and commits,
those two executions are referred to as two different "tasks" by the statistics.
The statistics gives you the number of such "tasks" that committed and aborted.
(This includes both tasks that aborted partway through execution,
and those that finished execution and were waiting in the commit queue when they were aborted.
If a task aborts, re-executes, and aborts again,
each time it starts execution is counted as a separate "task" in the statistics.)
Among all tasks that committed, and among all tasks that aborted,
there are also some statistics that indicate
the total number of cycles such tasks were in each of three states,
summed across all those dynamic task instances:
* "Idle", i.e., waiting in a task queue to be dispatched
* "Executed", i.e., occupying a core
* "Waiting", i.e., finished execution and occupying only a slot in the commit queue,
  waiting to find out if it will commit or abort.

It is often useful to look at the "Executed" cycles.
For example, by looking at the total number of Executed cycles from tasks that commit,
which is shown at the bottom of the column of numbers for "Executed" under "Committed Cycles",
you know the total number of core-cycles that were spent doing useful work.
If you divide this number by the total runtime, you know,
on average, at any given moment during the simulation,
how many cores were doing useful work, which is a measure of how much parallelism Swarm managed to exploit.

### Monitoring in-progress simulations

While a simulation is running,
it also outputs some brief statistics to a file named "heartbeat",
which is updated every 10 seconds.
This can be used to monitor the progress of a long-running simulation
or judge how well it is performing while it is still running.

If you are running many simulations,
and particularly if you are running separate simulations in parallel,
it is useful to create a separate directory within which you run each simulation,
so that the files generated by each simulation are kept separate.

### Detailed results

Detailed results and statistics are dumped to several output files generated by the simulator.
sim.out contains all the statistics in a raw, human-readable text format.

For example, say you wanted to find cache hit/miss rates.
The data is available in sim.out sorted by type of cache ("l1d", "l2", etc.)
and then reported separately for each cache or cache bank:
for example, there may be one "l1d" cache per core, one "l2" cache per tile, and one "l3" bank per tile.

Every time the simulator runs, it also generates a few .h5 files, in the HDF5 binary format.
The same data from "sim.out" is saved in the file named "sim.h5".
There is another generated file "sim-cmp.h5", which holds similar data,
but compacted by aggregating values from similar objects,
e.g., it will report the total statistics across all L1D caches,
the total hits/misses across all L2 caches, and the total hits/misses in the L3 cache, not broken down by bank.

Note: for L1D caches,
you should add together the values of the fhGETS and hGETS counters to get the number of read hits,
and add the values of fhGETX and hGETX to get the number of write hits.

We often write Python scripts using the h5py library to gather data from the .h5 files,
as reading from the compact binary format is more efficient than parsing text.
It is not difficult to write a new Python script using the h5py library.
Alternatively, if you have installed the HDF5 command line tools
(from the `hdf5-tools` Debian/Ubuntu package, or in `$HDF5PATH/bin/` if you
built HDF5 from source when you set up your environment to run the simulator),
you can also convert the contents of an .h5 file into a text format using the `h5dump` command.

### Time series plots

TODO: explain how to use the `periodicStatsInterval` configuration parameter
and the anl.py script to generate time series graphs showing queue occupancies,
cycle breakdowns, and commit/abort rates.

### Overheads vs. scalability

In parallel systems, it is often important to measure the trade-off between
doing computations in a way that is work-efficient and therefore uses the least total resources such as energy,
and doing computation in a way that parallelizes well, so its performance can be improved by using many cores.

When interpreting the performance of a Swarm program,
it is important to try running the program on a 1-core system, where all tasks will be run serially,
as well as on a system with more cores.
By observing the performance on 1-core, you can evaluate constant-factor overheads
and how much work is being done by the program.
By observing how performance varies as you run the same program on a Swarm system with more cores,
you measure scalability.

An easy way to obtain some configuration files for different sorts of Swarm systems is to run the script
```
./scripts/configs.py
```
in the simulator repo, which will look at the current contents of sample.cfg
and use that as a base upon which it will apply many different variations
to generate a whole bunch of configuration files in a new directory named `configs`.
This will include files named like "1c.cfg", "16c.cfg", "64c.cfg", etc.
for Swarm systems with different numbers of cores.
Because this script uses sample.cfg as a base, you could temporarily modify sample.cfg
to, for example, use a different type of core model such as the "Scoreboard" or "OoO" model
before running the config-generation script,
and then the resulting directory of generated config files will all use your chosen core model.

Low-level Swarm programming
---------------------------

Here we discuss manually writing Swarm tasks, without the benefits of SCC/T4
or any other Swarm-specific compiler.

The [system tests](../tests) directory contains example Swarm programs
that also verify their own results so that they can act as tests for the simulator.
They also act as simple demonstrations of the low-level Swarm API.
Swarm programs use APIs exposed in header files
[in the runtime submodule](https://github.com/SwarmArch/runtime/tree/master/include/swarm).
These header files include comments on API usage.
In some cases, you must read simulator source code to see how some Swarm features work.
(For more details, you may wish to reference the [Developer Guide](DevGuide.mdown).)

We will examine one of these programs: [bfs.cpp](../tests/bfs.cpp).
Note this simple BFS implementation is just for demonstrating Swarm programming:
it uses a simple but inefficient graph representation that suffers false sharing,
and it is poorly optimized for load balance on graphs with high-degree nodes.

The `main()` function starts by generating a random graph
(with the size of the graph determined by command line arguments),
and ends with a fairly standard serial implementation of BFS written in ordinary C++,
and a check of whether the serial BFS produced the same results as the Swarm tasks.
All of that non-task code runs as ordinary serial C++.
All of the action of Swarm tasks comes from two function calls in `main`:
* `swarm::enqueue` spawns a task, sending it to the task queue(s) to be run later
* `swarm::run()` starts running the tasks, and does not return until all tasks have run
  (i.e., the task queues are completely empty)
The simulator reports the execution time of `swarm::run()`,
which tells you how much time was spent to execute the parallel version of BFS.

`swarm::enqueue` works similarly to what is described in the Swarm hardware papers:
* first argument must be the name of a function (e.g., `bfsTask`)
* second argument is a 64-bit integer timestamp (e.g., 1)
* The third argument is [a struct](https://github.com/SwarmArch/runtime/blob/fdf6137eb5d26cff2b06bdc2bb896984ddd170f1/include/swarm/impl/types.h#L41-L62) consisting of (optionally) a 64-bit integer spatial hint (see MICRO'16 paper) and some flags that convey information about the task (e.g., see MICRO'18 paper). The allowable values of the flags are defined in [enqflags.h](https://github.com/SwarmArch/runtime/blob/master/include/swarm/impl/enqflags.h#L29). New Swarm users can get by without worrying about most of these flags: it is safe to just pass `EnqFlags::NOHINT` to indicate you are not specifying a spatial hint. You can later use other flags to give hardware more information about the task to optimize its execution.
* Any additional arguments are just passed through to the task function

Every task must be associated with a function.
Such task functions must take a 64-bit integer timestamp as their first argument,
and may any number of additional arguments.

Under the hood, Swarm hardware can transfer
at most five 64-bit general-purpose register values to a task,
in addition to the timestamp.
So as long as each argument is at most 64 bits and there are at most 5 such arguments beyond the timestamp,
the implementation of `swarm::enqueue` will just [pass each argument in a 64-bit general-purpose register](https://github.com/SwarmArch/runtime/blob/fdf6137eb5d26cff2b06bdc2bb896984ddd170f1/include/swarm/impl/hwtasks.h#L415-L421).
Such a task is called a "bare runner".
Otherwise, if you pass more than 5 arguments or one of the arguments will not fit in 64 bits (8 bytes),
then the implementation of `swarm::enqueue` must use some other strategy to pass the arguments to the task.
If the total size of those arguments sums to no more than 40 bytes, the implementation of `swarm::enqueue` will automatically [pack argument values into up to 5 registers](https://github.com/SwarmArch/runtime/blob/fdf6137eb5d26cff2b06bdc2bb896984ddd170f1/include/swarm/impl/hwtasks.h#L429-L438).
We sometimes call this a "reg runner" or a "regTupleRunner".
If the arguments sum to more than 40 bytes, the implementation of `swarm::enqeueue` will [allocate a heap chunk to store the arguments](https://github.com/SwarmArch/runtime/blob/fdf6137eb5d26cff2b06bdc2bb896984ddd170f1/include/swarm/impl/hwtasks.h#L462-L465),
and just pass a pointer to the task, which must [read all the arguments and then free the memory](https://github.com/SwarmArch/runtime/blob/fdf6137eb5d26cff2b06bdc2bb896984ddd170f1/include/swarm/impl/hwtasks.h#L388-L390).
This is called a "mem runner" or "memTupleRunner".
This heap allocation is expensive and should be avoided when possible.

For example, `bfsTask` takes only one additional argument after its timestamp, which is a `Vertex*`,
so the task will be a bare runner. This is indicated by the "[B]" in the output of the simulator.

Some of the programs in the 'tests/' directory limit the maximum number of arguments for a task function to less than 5.
This value is configured through the [Sconscript](../tests/SConscript#L48)

Every Swarm program must include the `"swarm/api.h"` header, which defines `swarm::enqueue` and some other APIs:
https://github.com/SwarmArch/runtime/blob/master/include/swarm/api.h

Many Swarm programs, including bfs.cpp, also include `"swarm/algorithm.h"`,
which defines some `swarm::enqueue_all` template functions
that are crucial to writing performant Swarm programs.
See the documentation here: https://github.com/SwarmArch/runtime/blob/master/include/swarm/algorithm.h#L37
And see how bfs.cpp uses `swarm::enqueue_all` [here](../tests/bfs.cpp#L98-L101).
Note that, in place of the four-line call to `swarm::enqueue_all`,
it would be legal to write a simple range-based `for` loop like this:
```c++
for (Vertex* n : v->adj)
    swarm::enqueue(bfsTask, level, EnqFlags::NOHINT, n);
```
This would give the same results as the `swarm::enqueue_all`, but with worse performance.
When new users write their first Swarm programs,
it may be easier to first just write and debug the program with simple `for` loops like this.
After you have that working, you can replace all `for` loops that spawn tasks
with calls to `swarm::enqueue_all` to improve performance.

#### Programming with Fractal

Fractal extends Swarm's notion of timestamp ordering with the concept of nested domains.
Section 3 of the Fractal paper \[[ISCA 2017](https://doi.acm.org/10.1145/3079856.3080218)\]
explains the concept of domains in the Fractal execution model.
However, the current version of the simulator provides a programming interface
that is a bit different from what is described in the paper's subsection 3.1.

Instead of using `fractal::create_subdomain`, subdomains are created using `swarm::deepen`.
After a task calls `swarm::deepen`, its subsequent calls to `swarm::enqueue` spawn children
to the newly created subdomain by default.  Thus, there is no need for `enqueue_sub`.
After calling `swarm::deepen` and spawning children to a subdomain,
a task can call `swarm::undeepen` to change its default target domain back to its original domain,
so subsequent calls to `swarm::enqueue` spawn children to the same domain as the parent task.
Note that each task can only call `swarm::deepen` at most once.

Additionally, instead of using an `enqueue_super` interface as described in the paper,
we can use the flag `EnqFlag::PARENTDOMAIN` with `swarm::enqueue`.
The `PARENTDOMAIN` flag spawns a task to the nearest enclosing domain outside of the current default target domain.
That is, if a task is deepened and then uses `PARENTDOMAIN`, the child is spawned to the parent's original domain.
Otherwise, if a task is undeepened, `PARENTDOMAIN` spawns the child to the superdomain of the parent's domain.

Finally, in the current implementation, all domains are ordered with 64-bit timestamps.
You can always simulate a unordered domain by giving a single identical timestamp value
(e.g., timestamp zero) to all tasks in that domain.

You can think of deepening as moving a task into the subdomain it creates,
where it acts as if it has timestamp zero so it is ordered at the start of the subdomain.
In this view, `swarm::enqueue` always defaults to spawning children to the domain that
the task currently occupies, while the `PARENTDOMAIN` flag spawns children to the superdomain.
When a task is deepened, `swarm::timestamp` returns zero instead of the task's original timestamp.
Meanwhile, `swarm::superTimestamp` always returns the original timestamp of
the task that created the current default target domain.
This means, when called from a deepened context, `swarm::superTimestamp` returns
the original timestamp of the task that deepened, since this was the timestamp
in the superdomain at which the current domain was created.

### Debug with Swarm's sequential runtime

If a Swarm program crashes or gives incorrect results when run on the simulator,
you should first try to understand any error messages from the simulator.
We also provide an alternative software runtime that does not depend on the simulator,
which may make it easier to use debuggers (e.g., GDB) on your Swarm program,
or to determine if the problem is due to a simulator bug.
This sequential runtime uses ordinary priority queues to run tasks one at a time.
By default, our SCons build system already builds multiple versions of each test program
using the sequential runtime: one version is linked against `simalloc`,
a malloc implementation that depends on simulator support to avoid aborts,
and one uses your system's native malloc implementation (e.g., glibc's version of ptmalloc).
Only the latter version can be run without the simulator.
For example, you can run the BFS program without using the simulator as follows:
```
scons -j4 tests
./build/opt/tests/seq/native/bfs 2000 8
```

Simulation Traces
-----------------

The simulator can be built in an alternate mode that produces a detailed trace
of events such as enqueues, dequeues, and aborts.  You can build and run it like so:
```
scons -j4 --trace sim
./build/opt/trace_sim/sim -config sample.cfg -- ./build/opt/tests/swarm/bfs 2000 8
```
This tracing version of the simulator produces a file named "trace.out" every time it runs.
Note this log of events may be large for long-running simulations.
To extract useful information from this trace, we have provided three Python scripts:
 - [trace.py](../scripts/trace.py) analyzes and reports parallelism and causes of aborts.
 - [taskTimeline.py](../scripts/taskTimeline.py) renders a plot of how each core spent its time.
 - [graphEnqueues.py](../scripts/graphEnqueues.py) shows which tasks spawned other tasks.

### Understanding abort causes

In addition to reporting some statistics similar to those printed to the
terminal by the simulator itself (e.g., how many tasks committed and aborted,
how many core-cycles were spent executing tasks, the number of cycles spent
executing each task), trace.py prints information about which individual
memory-accessing instructions triggered aborts.

There are several types of task aborts:
 - Memory-dependence aborts (a.k.a. data aborts): a memory access matches
   the readset or writeset of a higher-timestamp task, which is aborted.
 - Resource aborts: a high-timestamp task is aborted to free a hardware
   resource (e.g., a commit queue entry) needed to enable execution
   of a lower-timestamp task, to avoid priority inversion and deadlock.
   See the MICRO'15 paper's Section 4.7: "Handling Limited Queue Sizes".
   Resource aborts are usually rare in code that is well parallelized.
   Resource aborts commonly occur in code with limited parallelism, when some
   cores attempt to run far-ahead tasks, that must wait a long time to commit,
   thus filling up commit queues.
 - Exceptions: when speculation is unsafe or undesirable for some task's action,
   the task may be aborted.  See discussion of exceptions and promoting tasks
   in the MICRO'18 paper, specifically Section 3.C and the end of Section 5.B.
   `swarm::serialize()` is the API that corresponds what the paper calls the
   `promote` instruction, which requests an exception abort.
 - Fractal zooming aborts: During zoom-in, base domain tasks may be aborted
   and spilled (as described in the ISCA'17 paper's Section 4.3: "Supporting
   unbounded nesting").  During zoom-out, too-deep domains may be aborted.
   Zooming aborts can occur only in applications that make use of Fractal.
 - Parent-child aborts: children tasks are aborted when notified that their
   parents aborted.  Unlike other abort types, which result in requeuing the
   victim task for later execution, parent-child aborts result in the child
   task being discarded.

The output of trace.py distinguishes between "direct" aborts and
abort cascades where the aborts of dependant tasks are indirectly caused by
whatever event caused the first task in the cascade to abort.  Specifically,
the root cause of an abort cascade is always a resource abort, zooming abort,
or by a memory-dependence abort triggered by an ordinary memory access.
After the cascade starts, each victim task notifies its children,
causing indirect parent-child aborts, and tasks dependent on data written by
a victim task are also indirectly aborted when the victim undoes its writes.
In the output of trace.py, the cycles of execution wasted on both directly and
indirectly aborted tasks are attributed to the event that started the cascade.
DCycles is the number of core-cycles spent executing the directly aborted victim task.
TCycles additionally includes cycles wasted executing tasks dependant on the victim.
